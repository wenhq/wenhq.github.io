---
layout: post
title: "七个用于数据科学(data science)的命令行工具"
date: '2014-01-01T13:36:00.001+08:00'
author: Wenh Q
tags: 
modified_time: '2014-01-01T13:36:19.084+08:00'
blogger_id: tag:blogger.com,1999:blog-4961947611491238191.post-7186911659259341526
blogger_orig_url: http://binaryware.blogspot.com/2014/01/data-science.html
---

<div dir="ltr"><table cellpadding="0" cellspacing="0" style="width: 650pxpx;"><tbody><tr><td><div style="margin-top: 15px;">通过 <a href="http://blog.jobbole.com/" style="font-size: 13px;" target="_blank">博客 - 伯乐在线</a>              </div><div dir="ltr" style="font-size: 14px; line-height: 20px; margin-top: 15px;">数据科学是<a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/" rel="nofollow" target="_blank">OSEMN</a>（和 awesome 相同发音），它包括获取(Obtaining)、整理(Scrubbing)、探索(Exploring)、建模(Modeling)和翻译(iNterpreting)数据。作为一名数据科学家，我用命令行的时间非常长，尤其是要获取、整理和探索数据的时候。而且我也不是唯一一个这样做的人。最近，<a href="http://www.gregreda.com/2013/07/15/unix-commands-for-data-science/" rel="nofollow" target="_blank">Greg Reda</a>介绍了可用于数据科学的经典命令行工具。在这之前，Seth Brown介绍了如何<a href="http://www.drbunsen.org/explorations-in-unix/" rel="nofollow" target="_blank">在Unix下进行探索性的数据分析</a>。<br />下面我将介绍在我的日常工作中发现很有用的七个命令行工具。包括：<a href="http://stedolan.github.io/jq/" rel="nofollow" target="_blank">jq</a>、 <a href="https://github.com/jehiah/json2csv" rel="nofollow" target="_blank">json2csv</a>、 <a href="https://github.com/onyxfish/csvkit" rel="nofollow" target="_blank">csvkit</a>、scrape、 <a href="https://github.com/parmentf/xml2json" rel="nofollow" target="_blank">xml2json</a>、 sample 和 Rio。(我自己做的scrape、sample和Rio可以在<a href="https://github.com/jeroenjanssens/data-science-toolbox" rel="nofollow" target="_blank">这里</a>拿到)。任何建议意见、问题甚至git上的拉取请求都非常欢迎(其他人建议的工具可以在最后找到)。好的，下面我们首先介绍jq。<br /><h3>1. jq – sed for JSON</h3>JSON现在越来越流行，尤其当API盛行了以后。我还记得处理JSON时，用grep和sed写着丑陋的代码。谢谢jq，终于可以不用写的这么丑了。<br />假设我们对2008总统大选的所有候选人感兴趣。纽约时报有一个关于<a href="http://developer.nytimes.com/docs/campaign_finance_api/" target="_blank" title="API">竞选财务的API</a>。让我们用curl取一些JSON:<br /><pre>curl -s '<a href="http://api.nytimes.com/svc/elections/us/v3/finances/2008/president/totals.json?api-key=super-secret" target="_blank">http://api.nytimes.com/svc/elections/us/v3/finances/2008/president/totals.json?api-key=super-secret</a>' &gt; nyt.json</pre>-s表示静默模式。然后我们用jq最简单的格式jq '.'，可以把得到的丑陋的代码<br /><pre>{"status":"OK","base_uri":"<a href="http://api.nytimes.com/svc/elections/us/v3/finances/2008/" target="_blank">http://api.nytimes.com/svc/elections/us/v3/finances/2008/</a>","cycle":2008,"copyright":"Copyright (c) 2013 The New York Times Company. All Rights Reserved.","results":[{"candidate_name":"Obama, Barack","name":"Barack Obama","party":"D",</pre>转换成漂亮的格式：<br /><pre>&lt; nyt.json jq '.' | head { "results": [ { "candidate_id": "P80003338", "date_coverage_from": "2007-01-01", "date_coverage_to": "2008-11-24", "candidate_name": "Obama, Barack", "name": "Barack Obama", "party": "D",</pre>同时，jq还可以选取和过滤JSON数据：<br /><pre>&lt; nyt.json jq -c '.results[] | {name, party, cash: .cash_on_hand} | select(.cash | tonumber &gt; 1000000)'   {"cash":"29911984.0","party":"D","name":"Barack Obama"}  {"cash":"32812513.75","party":"R","name":"John McCain"}  {"cash":"4428347.5","party":"D","name":"John Edwards"}</pre>更多使用方法参见<a href="http://stedolan.github.io/jq/manual/" target="_blank">手册</a>，但是不要指望jq能做所有事。Unix的哲学是写能做一件事并且做得好的程序，但是jq功能强大！下面就来介绍json2csv。<br /><h3>2. json2csv – 把JSON转换成CSV</h3>虽然JSON适合交换数据，但是它不适合很多命令行工具。但是不用担心，用json2csv我们可以轻松把JSON转换成CSV。现在假设我们把数据存在million.json里，仅仅调用<br /><pre>&lt; million.json json2csv -k name,party,cash</pre>就可以把数据转换成：<br /><pre>Barack Obama,D,29911984.0  John McCain,R,32812513.75  John Edwards,D,4428347.5</pre>有了CSV格式我们就可以用传统的如 cut -d 和 awk -F 一类的工具了。grep和sed没有这样的功能。因为CSV是以表格形式存储的，所以csvkit的作者开发了csvkit。<br /><h3>3. csvkit – 转换和使用CSV的套装</h3>csvkit不只是一个程序，而是一套程序。因为大多数这类工具"期望"CSV数据有一个表头，所以我们在这里加一个。<br /><pre>echo name,party,cash | cat - million.csv &gt; million-header.csv</pre>我们可以用csvsort给候选人按竞选资金排序并展示：<br /><pre>&lt; million-header.csv csvsort -rc cash | csvlook    |---------------+-------+--------------|  |  name         | party | cash         |  |---------------+-------+--------------|  |  John McCain  | R     | 32812513.75  |  |  Barack Obama | D     | 29911984.0   |  |  John Edwards | D     | 4428347.5    |  |---------------+-------+--------------|</pre>看起来好像MySQL哈？说到数据库，我们可以把CSV写到sqlite数据库（很多其他的数据库也支持）里，用下列命令：<br /><pre>csvsql --db sqlite:///myfirst.db --insert million-header.csv  sqlite3 myfirst.db  sqlite&gt; .schema million-header  CREATE TABLE "million-header" (      name VARCHAR(12) NOT NULL,       party VARCHAR(1) NOT NULL,       cash FLOAT NOT NULL  );</pre>插入后数据都会正确因为CSV里也有格式。此外，这个套装里还有其他有趣工具，如 in2csv、 csvgrep 和csvjoin。通过csvjson，数据甚至可以从csv转换会json。总之，你值得一看。<br /><h3>4. scrape – 用XPath和CSS选择器进行HTML信息提取的工具</h3>JSON虽然很好，但是同时也有很多资源依然需要从HTML中获取。scrape就是一个Python脚本，包含了lxml和cssselect包，从而能选取特定HTML元素。维基百科上有个网页列出了所有国家的边界线语国土面积的比率，下面我们来把比率信息提取出来吧<br /><pre>curl -s '<a href="http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio" target="_blank">http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio</a>' | scrape -b -e 'table.wikitable &gt; tr:not(:first-child)' | head  &lt;!DOCTYPE html&gt;  &lt;html&gt;  &lt;body&gt;  &lt;tr&gt;  &lt;td&gt;1&lt;/td&gt;  &lt;td&gt;Vatican City&lt;/td&gt;  &lt;td&gt;3.2&lt;/td&gt;  &lt;td&gt;0.44&lt;/td&gt;  &lt;td&gt;7.2727273&lt;/td&gt;  &lt;/tr&gt;</pre>-b命令让scrape包含和标签，因为有时xml2json会需要它把HTML转换成JSON。<br /><h3>5. xml2json – 把XML转换成JSON</h3>如名字所说，这工具就是把XML(HTML也是一种XML)转换成JSON的输出格式。因此，xml2json是连接scrape和jq之间的很好的桥梁。<br /><pre>curl -s '<a href="http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio" target="_blank">http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio</a>' | scrape -be 'table.wikitable &gt; tr:not(:first-child)' | xml2json | jq -c '.<a href="http://html.body.tr/" target="_blank">html.body.tr</a>[] | {country: .td[1][], border: .td[2][], surface: .td[3][], ratio: .td[4][]}' | head  {"ratio":"7.2727273","surface":"0.44","border":"3.2","country":"Vatican City"}  {"ratio":"2.2000000","surface":"2","border":"4.4","country":"Monaco"}  {"ratio":"0.6393443","surface":"61","border":"39","country":"San Marino"}  {"ratio":"0.4750000","surface":"160","border":"76","country":"Liechtenstein"}  {"ratio":"0.3000000","surface":"34","border":"10.2","country":"Sint Maarten (Netherlands)"}  {"ratio":"0.2570513","surface":"468","border":"120.3","country":"Andorra"}  {"ratio":"0.2000000","surface":"6","border":"1.2","country":"Gibraltar (United Kingdom)"}  {"ratio":"0.1888889","surface":"54","border":"10.2","country":"Saint Martin (France)"}  {"ratio":"0.1388244","surface":"2586","border":"359","country":"Luxembourg"}  {"ratio":"0.0749196","surface":"6220","border":"466","country":"Palestinian territories"}</pre>当然JSON数据之后可以输入给json2csv。<br /><h3>6. sample – 用来debug</h3>我写的第二个工具是sample。（它是依据bitly的<a href="https://github.com/bitly/data_hacks" target="_blank">data_hacks</a>写的，bitly还有好多其他工具值得一看。）当你处理大量数据时，debug管道非常尴尬。这时，sample就会很有用。这个工具有三个用处：<br /><ol><li>逐行展示数据的一部分。</li><li>给在输出时加入一些延时，当你的数据进来的时候有些延时，或者你输出太快看不清楚时用这个很方便。</li><li>限制程序运行的时间。</li></ol>下面的例子展现了这三个功能：<br /><pre>seq 10000 | sample -r 20% -d 1000 -s 5 | jq '{number: .}'</pre>这表示，每一行有20%的机会被给到jq，没两行之间有1000毫秒的延迟，5秒过后，sample会停止。这些选项都是可选的。为了避免不必要的计算，请尽早sample。当你debug玩之后你就可以把它移除了。<br /><h3>7. Rio – 在处理中加入R</h3>这篇文章没有R就不完整。将R/Rscript加入处理不是很好理解，因为他们并没有标准化输入输出，因此，我加入了一个命令行工具脚本，这样就好理解了。<br />Rio这样工作：首先，给标准输入的CSV被转移到一个临时文件中，然后让R把它读进df中。之后，在-e中的命令被执行。最后，最后一个命令的输出被重定向到标准输出中。让我用一行命令展现这三个用法，对每个部分展现5个数字的总结：<br /><pre>curl -s '<a href="https://raw.github.com/pydata/pandas/master/pandas/tests/data/iris.csv" target="_blank">https://raw.github.com/pydata/pandas/master/pandas/tests/data/iris.csv</a>' &gt; iris.csv  &lt; iris.csv Rio -e 'summary(df)'    SepalLength      SepalWidth     PetalLength      PetalWidth      Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100     1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300     Median :5.800   Median :3.000   Median :4.350   Median :1.300     Mean   :5.843   Mean   :3.054   Mean   :3.759   Mean   :1.199     3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800     Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500         Name             Length:150           Class :character     Mode  :character</pre>如果加入了-s选项，sqldf包会被引入，这样CSV格式就会被输出，这可以让你之后用别的工具处理数据。<br /><pre>&lt; iris.csv Rio -se 'sqldf("select * from df where df.SepalLength &gt; 7.5")' | csvlook  |--------------+------------+-------------+------------+-----------------|  |  SepalLength | SepalWidth | PetalLength | PetalWidth | Name            |  |--------------+------------+-------------+------------+-----------------|  |  7.6         | 3          | 6.6         | 2.1        | Iris-virginica  |  |  7.7         | 3.8        | 6.7         | 2.2        | Iris-virginica  |  |  7.7         | 2.6        | 6.9         | 2.3        | Iris-virginica  |  |  7.7         | 2.8        | 6.7         | 2          | Iris-virginica  |  |  7.9         | 3.8        | 6.4         | 2          | Iris-virginica  |  |  7.7         | 3          | 6.1         | 2.3        | Iris-virginica  |  |--------------+------------+-------------+------------+-----------------|</pre>如果你用-g选项，ggplot2会被引用，一个叫g得带有df的ggplot对象会被声明。如果最终输出是个ggplot对象，一个PNG将会被写到标准输出里。<br /><pre>&lt; iris.csv Rio -ge 'g+geom_point(aes(x=SepalLength,y=SepalWidth,colour=Name))' &gt; iris.png</pre><img alt="iris" src="http://jbcdn2.b0.upaiyun.com/2014/01/iris.png" /><br />我制作了这个工具，为了可以在命令行中充分利用R的力量。当然它有很多缺陷，但至少我们不需要再学习gnuplot了。<br /><b>别人建议的命令行工具</b><br />下面是其他朋友通过twitter和hacker news推荐的工具，谢谢大家。<br /><ul><li><a href="http://bigmler.readthedocs.org/en/latest/" target="_blank">BigMLer</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=aficionado" target="_blank">aficionado</a></li><li><a href="https://code.google.com/p/crush-tools/" target="_blank">crush-tools</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=mjn" target="_blank">mjn</a></li><li><a href="https://github.com/dergachev/csv2sqlite" target="_blank">csv2sqlite</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=dergachev" target="_blank">dergachev</a></li><li><a href="https://github.com/dbro/csvquote" target="_blank">csvquote</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=susi22" target="_blank">susi22</a></li><li><a href="https://github.com/clarkgrubb/data-tools" target="_blank">data-tools repository</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=cgrubb" target="_blank">cgrubb</a></li><li><a href="https://github.com/dkogan/feedgnuplot" target="_blank">feedgnuplot</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=dima55" target="_blank">dima55</a></li><li><a href="https://github.com/cgutteridge/Grinder/tree/master/bin" target="_blank">Grinder repository</a>&nbsp;by&nbsp;<a href="https://twitter.com/cgutteridge" target="_blank">@cgutteridge</a></li><li><a href="http://www.hdfgroup.org/HDF5/doc/RM/Tools.html" target="_blank">HDF5 Tools</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=susi22" target="_blank">susi22</a></li><li><a href="http://code.google.com/p/littler/" target="_blank">littler</a>&nbsp;by&nbsp;<a href="https://twitter.com/eddelbuettel" target="_blank">@eddelbuettel</a></li><li><a href="http://gibrown.wordpress.com/2013/01/26/unix-bi-grams-tri-grams-and-topic-modeling/" target="_blank">mallet</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=gibrown" target="_blank">gibrown</a></li><li><a href="https://github.com/benbernard/RecordStream" target="_blank">RecordStream</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=revertts" target="_blank">revertts</a></li><li><a href="https://github.com/paulgb/subsample" target="_blank">subsample</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=paulgb" target="_blank">paulgb</a></li><li><a href="http://search.cpan.org/%7Eken/xls2csv-1.07/script/xls2csv" target="_blank">xls2csv</a>&nbsp;by&nbsp;<a href="https://twitter.com/sheeshee" target="_blank">@sheeshee</a></li><li><a href="http://xmlstar.sourceforge.net/" target="_blank">XMLStarlet</a>&nbsp;by&nbsp;<a href="https://news.ycombinator.com/user?id=gav" target="_blank">gav</a></li></ul><b>结论</b><br />我介绍了七个我日常用来处理数据的命令行工具。虽然每个工具各有所长，我经常是将它们与传统工具（如grep, sed, 和awk）一起使用。将小工具结合起来使用组成一个大的流水线，这就是其用处所在。<br />不知你们对这个列表有什么想法，你们平时喜欢用什么工具呢。如果你们也做了什么好玩的工具，欢迎将其加入数据科学工具包<a href="https://github.com/jeroenjanssens/data-science-toolbox" target="_blank">data science toolbox</a>。<br />如果你不认为自己能制作工具，也不用担心，下次当你写一个异乎寻常的命令行流水线时，记得将它放到一个文件里，加一个#!，加一些参数，改成可执行文件，你就做成一个工具啦~<br />虽然命令行工具的强大在获取、处理和探索数据时不容小觑，在真正的探索、建模和理解翻译数据时，你还是最好在科学计算环境下进行。比如<a href="http://www.r-project.org/" target="_blank">R</a>或者<a href="http://ipython.org/notebook.html" target="_blank">IPython notebook</a>+<a href="http://pandas.pydata.org/" target="_blank">pandas</a>。<br />如果感兴趣，欢迎<a href="https://twitter.com/jeroenhjanssens/" target="_blank">follow me on Twitter</a>。<br /><div><div><h3>相关文章</h3><ul><li><a href="http://blog.jobbole.com/29561/" target="_blank">Google Dremel 原理 – 如何能3秒分析1PB</a></li><li><a href="http://blog.jobbole.com/46839/" target="_blank">海量数据相似度计算之simhash和海明距离</a></li><li><a href="http://blog.jobbole.com/13178/" target="_blank">大数据时代</a></li><li><a href="http://blog.jobbole.com/13538/" target="_blank">Hadoop：你不得不了解的大数据工具</a></li><li><a href="http://blog.jobbole.com/52898/" target="_blank">使用大数据时，别忘了关注Linux内存管理器</a></li><li><a href="http://blog.jobbole.com/12720/" target="_blank">数据挖掘：如何寻找相关项</a></li><li><a href="http://blog.jobbole.com/46673/" target="_blank">社会化海量数据采集爬虫框架搭建</a></li><li><a href="http://blog.jobbole.com/39489/" target="_blank">将安全开发流程扩展到云和大数据</a></li><li><a href="http://blog.jobbole.com/46664/" target="_blank">低成本服务器搭建千万级数据采集系统</a></li><li><a href="http://blog.jobbole.com/51177/" target="_blank">Presto：Facebook的分布式SQL查询引擎</a></li></ul></div></div><a href="http://blog.jobbole.com/54308/" target="_blank">七个用于数据科学(data science)的命令行工具</a>，首发于<a href="http://blog.jobbole.com/" target="_blank">博客 - 伯乐在线</a>。</div></td><td style="background-color: #f2f2f2; color: #303030; padding: 20px 40px; text-align: left;"></td></tr></tbody></table></div>